joints_renderer:
  _target_: src.renderer.matplotlib.MatplotlibRender
  jointstype: "guoh3djoints"
  fps: 20.0
  colors: ['black', 'magenta', 'red', 'green', 'blue']
  figsize: 4
  canonicalize: true
  radius: 4.0
  fontsize: 10

smpl_renderer:
  _target_: src.renderer.humor.HumorRenderer
  fps: 20.0
  imw: 720
  imh: 720

diffusion:
  weight: 1.0
  mcd: True

  denoiser:
    dropout: 0.0


# General parameters
num_gen_per_prompt: 4
num_prompts_dataset:  128
num_workers: 6

# Training parameters
iterations: 30000
train_epochs: 3 #4
train_batch_size: 12
grad_clip: 1.0
advantage_clip_epsilon: 1e-4

# Reward Model
reward: "TMR++"
masking_ratio: 0.75
reward_scale: 10

# Sequence parameters
sequence_fixed: false
fps: 20
#time: 2.5
#joint_stype: "both"  # Can be either "both" or "smpljoints"

# Optimizer parameters
lr: 1e-5
beta1: 0.9
beta2: 0.999
eps: 1e-8
weight_decay: 1e-4

# Loss parameters
betaL: 0
alphaL: 1

# Validation/Test parameters
val_iter: 25
val_batch_size: 512
val_num_batch: 0
render_videos: false

#Layer freeze
freeze_normalization_layers: true

#LorA
lora: false
lora_rank: 4
lora_alpha: 16
lora_dropout: 0.0
lora_bias: "none"

# WanDB parameters
experiment_name: 'New_'
group_name: ''

#guidance
guidance_weight: 1.0

#generate options
opt_path: './checkpoints/t2m/t2m_condunet1d_batch64/opt.txt'  # option file path for loading model
gpu_id: 0  # GPU id
output_dir: ''  # Directory path to save generation result
footskate_cleanup: false  # Whether to use footskate cleanup in inference

# inference
num_inference_steps: 10  # Number of iterative denoising steps during inference
which_ckpt: 'latest'  # name of checkpoint to load
diffuser_name: 'ddpm'  # sampler's scheduler class name in the diffuser library
use_ema: false  # use EMA model in inference
seed: 0

debug: false
real_dataset_name: "t2m"